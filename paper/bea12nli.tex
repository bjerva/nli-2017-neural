%\title{emnlp 2017 instructions}
% File emnlp2017.tex
%

\documentclass[11pt,letterpaper]{article}
\usepackage{emnlp2017}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}

\emnlpfinalcopy % System descriptions should not be anonymous

%  Enter the EMNLP Paper ID here:
%\def\emnlppaperid{***}

% To expand the titlebox for more authors, uncomment
% below and set accordingly.
% \addtolength\titlebox{.5in}

\newcommand\BibTeX{B{\sc ib}\TeX}


\title{Neural Networks and Spelling Features for Native Language Identification}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}
% If the title and author information does not fit in the area allocated,
% place \setlength\titlebox{<new height>} right after
% at the top, where <new height> can be something larger than 2.25in
%\author{Johannes Bjerva$^1$ \and Gintar\.e Grigonyt\.e$^2$ \and Robert {\"O}stling$^2$ \and Barbara Plank$^1$ \\
%{\tt j.bjerva@rug.nl\hfill{gintare,robert}@ling.su.se\hfill b.plank@rug.nl}\\
%{University of Groningen$^1$\hspace{4cm}Stockholm University$^2$}}

\author{Johannes Bjerva \\ CLCG \\ University of Groningen \\ {\tt j.bjerva@rug.nl}
        \And Gintar\.e Grigonyt\.e \\ Department of Linguistics \\ Stockholm University \\ {\tt gintare@ling.su.se}
        \AND Robert {\"O}stling \\ Department of Linguistics \\  Stockholm University \\ {\tt robert@ling.su.se}
        \And Barbara Plank \\ CLCG \\ University of Groningen \\ {\tt b.plank@rug.nl}
}
        %}% } \And
        % Author n \\ Address line \\ ... \\ Address line}

\date{}

\begin{document}

\maketitle

\begin{abstract}
    We present the RUG-SU team's submission at the Native
    Language Identification Shared Task 2017.
    We combine several approaches into an ensemble, based on spelling error features, a simple neural network using word representations, a deep residual network using word and character features, and a system based on a recurrent neural network. Our best system is an ensemble of neural networks, reaching an F1 score of 0.8323.
    Although our system is not the highest ranking one, we do outperform the baseline by far.
\end{abstract}


\section{Introduction}

Native Language Identification (NLI) is the task of identifying the native language of, e.g., the writer of an English text.
In this paper, we describe the University of Groningen / Stockholm University (team RUG-SU) submission to NLI Shared Task 2017 \citep{nli2017}.
Neural networks constitute one of the most popular methods in natural language processing these days \citep{manning:2016}, but appear not to have been previously used for NLI.
Our goal in this paper is therefore twofold.
On the one hand, we wish to investigate how well a neural system can perform the task.
On the other hand, we wish to investigate the effect of using features based on spelling errors.

\section{Related Work}

NLI is an increasingly popular task, which has been the subject of several shared tasks in recent years \citep{nli2013,compare2016,nli2017}.
Although earlier shared task editions have focussed on English, NLI has recently also turned to including non-English languages \citep{multilingual-nli}.
Additionally, although the focus in the past has been on using written text, speech transcripts and audio features have also been included in recent editions, for instance in the 2016 Computational Paralinguistics Challenge \citep{compare2016}.
Although these aspects are combined in the NLI Shared Task 2017, with both written and spoken responses available, we only utilise written responses in this work.
For a further overview of NLI, we refer the reader to \citet{malmasi2016}.

Previous approaches to NLI have used syntactic features \citep{bykh:2014}, string kernels \citep{ionescu:2014}, and variations of ensemble models \citep{malmasi:2017:nlisg,nli2013}.
No systems used neural networks in the 2013 shared task \citep{nli2013}, hence ours is one of the first works using a neural approach for this task, along with concurrent submissions in this shared task \citep{nli2017}.

\section{External data}

\subsection{PoS-tagged sentences}
We indirectly use the training data for the Stanford PoS tagger
\citep{Manning2014corenlp}, and for initialising word embeddings we use
GloVe embeddings from 840 billion tokens of web data.\footnote{\url{https://nlp.stanford.edu/projects/glove/}}

\subsection{Spelling features}
We investigate learner misspellings, which is mainly motivated by two assumptions.
For one, spelling errors are quite prevalent in learners' written production~\cite{kochmar2011}.
Additionally, spelling errors have been shown to be influenced by phonological L1 transfer \citep{grigonyte2014pronunciation}.
We use the Aspell spell checker to detect misspelled words.\footnote{\url{http://aspell.net}}

\section{Systems}

\setlength{\tabcolsep}{4.5pt}
\begin{table*}[htbp]
    \small
\centering
\caption{Official results for the essay task, with and without external resources (ext. res.).}
\label{tab:results}
\begin{tabular}{llrr}
\toprule
\bf Setting & \bf System & \bf F1 (macro) & \bf Accuracy \\
\midrule
\multirow{2}{*}{Baselines}& Random Baseline & 0.0909 & 0.0909 \\
& Official Baseline & 0.7100 & 0.7100 \\
\midrule
\multirow{6}{*}{No ext. res.} & 01 -- Resnet ($w_1$+$c_5$) & 0.8016 & 0.8027 \\
& 02 -- Resnet ($w_1$+$c_5$) & 0.7776 & 0.7782 \\
& 03 -- Ensemble (Resnet ($w_1$+$c_5$), Resnet ($c_4$)) & 0.7969 & 0.7964 \\
& 04 -- Ensemble (Resnet ($w_1$+$c_5$), Resnet ($c_6$), Resnet ($c_4$), Resnet ($c_3$)) & 0.8023 & 0.8018 \\
& 05 -- Ensemble (Resnet ($w_1$+$c_5$), Resnet ($c_6$), Resnet ($c_4$), CBOW) & 0.8149 & 0.8145 \\
& 06 -- Ensemble (Resnet ($w_1$+$c_5$), Resnet ($c_6$), MLP, CBOW) & \bf 0.8323 & \bf 0.8318 \\
\midrule
\multirow{6}{*}{With ext. res.} & 01 -- Ensemble (LSTM, Resnet ($w_1$+$c_5$)) & \bf 0.8191 & \bf 0.8186 \\
& 02 -- Ensemble (LSTM, Resnet ($w_1$+$c_5$), Resnet ($c_4$)) & 0.8191   &  0.8195 \\
& 03 -- Ensemble (Spell, LSTM, Resnet ($w_1$+$c_5$), Resnet ($c_6$), CBOW) & 0.8173   &  0.8175 \\
& 04 -- Ensemble (Spell, Resnet ($w_1$+$c_5$), Resnet ($c_6$), CBOW) & 0.8055   &  0.8051 \\
& 05 -- Ensemble (Spell, Spell, Resnet ($w_1$+$c_5$), Resnet ($c_6$), Resnet ($c_4$), CBOW) & 0.8045   &  0.8048 \\
& 06 -- Ensemble (LSTM, Resnet ($w_1$+$c_5$), Resnet ($c_6$), Resnet ($c_4$), CBOW)& 0.8009   &  0.8007 \\
\bottomrule
\end{tabular}
\end{table*}
\subsection{Deep Residual Networks}
Deep residual networks, or \textit{resnets}, are a class of convolutional neural networks, which consist of several convolutional blocks with skip connections in between \citep{resnets:2015,He2016identity}.
Such skip connections facilitate error propagation to earlier layers in the network, which allows for building deeper networks.
Although their primary application is image recognition and related tasks, recent work has found deep residual networks to be useful for a range of NLP tasks.
Examples of this include morphological re-inflection \citep{robert:sigmorphon:2016},
%language identification \citep{bjerva:2016:dsl},
semantic tagging \citep{bjerva:2016:semantic}, and other text classification tasks \citep{conneau:2016}.

We apply resnets with four residual blocks.
Each residual block contains two successive one-dimensional convolutions, with a kernel size and stride of $2$.
Each such block is followed by an average pooling layer and dropout ($p=0.5$, \citet{dropout}).
The resnets are applied to several input representations: word unigrams, and character $4$- to $6$-grams.
These input representations are first embedded into a $64$-dimensional space, and trained together with the task.
We do not use any pre-trained embeddings for this sub-system.
The outputs of each resnet are concatenated before passing through two fully connected layers, with $1024$ and $256$ hidden units respectively.
We use the rectified linear unit (ReLU, \citet{rectifier}) activation function.
We train the resnet over $50$ epochs with the Adam optimisation algorithm \citep{adam}, using the model with the lowest validation loss.
In addition to dropout, we use weight decay for regularisation ($\epsilon=10^{-4}$, \citet{weightdecay}).

\subsection{PoS-tagged sentences}
In order to easier capture general syntactic patterns, we use a sentence-level
bidirectional LSTM over tokens and their corresponding part of speech tags
from the Stanford CoreNLP toolkit \citep{Manning2014corenlp}.  PoS tags are
represented by
64-dimensional embeddings, initialised randomly;  word tokens by
300-dimensional embeddings, initialised with GloVe \citep{Pennington2014glove}
embeddings trained on 840 billion words of English web data from the Common
Crawl project.\footnote{\url{https://nlp.stanford.edu/projects/glove/}}

To reduce overfitting, we perform training by choosing a random subset of 50\%
of the sentences in an essay, concatenating their PoS tag and token
embeddings, and running the resulting vector sequence through a bidirectional
LSTM layer with 256 units per direction. We then average the final output
vector of the LSTM over all the selected sentences from the essay, pass it
through a hidden layer with 1024 units and rectified linear activations, then
make the final predictions through a linear layer with softmax activations.
We apply dropout ($p = 0.5$) on the final hidden layer.

\subsection{Spelling features}
Essays are checked with the Aspell spell checker for any misspelled words. If misspellings occur, we simply consider the first suggestion of the spell checker to be the most likely correction.
The features for NLI classification are derived entirely from misspelled words.
We consider deletion, insertion, and replacement type of corrections. Features are represented as pairs of original and corrected character sequences (uni, bi, tri), for instance:

\begin{verbatim}
visiters visitors
{(e,o),(te,to),(ter,tor)}
travellers travelers
{(l,0),(ll,l0),(ole,l0e)}
\end{verbatim}

These features are fed to a logistic regression classifier with builtin cross-validation, as implemented in the scikit-learn library.\footnote{\url{http://scikit-learn.org/}}

\begin{figure*}[ht]
\centering
\includegraphics[trim={8cm 0 0 0},clip,width=0.8\textwidth]{conf_matrix_run06closed}
\caption{Confusion matrix for our best run (closed track, run 06)}
\label{fig:conf_mat}
\end{figure*}

\subsection{CBOW features}
We complement the neural approaches with a simple neural network that uses word representations, namely a \textit{continuous bag-of-words} (CBOW) model~\cite{mikolov2013efficient}. It represents each essay simply as the average embedding of all words in the essay. The intuition is that this simple model provides complementary evidence to the models  that use sequential information. Our CBOW model was tuned on the \textsc{Dev} data and consists of an input layer of 512 input nodes, followed by a dropout layer ($p=0.1$) and a single softmax output layer. The model was trained for 20 epochs with Adam using a batch size of 50. No pre-trained embeddings were used in this model.
We additionally experiment with a simple multiplayer perceptron (MLP). In contrast to CBOW it uses $n$-hot features (of the size of the vocabulary), a single layer with 512 nodes, sigmoid activation and dropout ($p=0.1$). The remaining training parameters are the same as for CBOW. We see that this model adds complementary knowledge in the closed-track ensemble (run 06).

\subsection{Ensemble}
The systems are combined into an ensemble, consisting of a linear SVM.
We use the probability distributions over the labels, as output by each system, as features for the SVM, as in meta-classification \citep{malmasi:2017:nlisg}.
The ensemble is trained and tuned on a random subset of the development set ($70/30$ split).
For the selection of systems to include in the ensemble, we use the combination of systems resulting in the highest mean accuracy over five such random splits.


\section{Results}
The results when using external resources are lower than when not using them (Table~\ref{tab:results}).
Our best result without external resources is an F1 score of $83.23$, whereas we obtain F1 score of $81.91$ with such resources.
Figure~\ref{fig:conf_mat} shows the confusion matrix of our best system's predictions (run 06).
Most confusions occur in three groups: \textit{Hindi} and \textit{Telugu} (South Asian), \textit{Japanese} and \textit{Korean} (East Asian), and \textit{French}, \textit{Italian} and \textit{Spanish} (South European).


\section{Discussion}

In isolation, the ResNet system yields a relatively high F1 score of 80.16.
This indicates that, although simpler methods yield better results for this task, deep neural networks are also applicable.
However, further experimentation is needed before such a system can outperform the more traditional feature-based systems.
This is in line with previous findings for the related task of language identification \citep{medvedeva:2017,vardial2017}.
Combining all of our systems without external data yields an F1 score of 83.23, which places our system in the third best performing group of the NLI Shared Task 2017 \citep{nli2017}.

When adding external data, the best performing systems are those including the spelling system predictions and/or the LSTM predictions.
However, the highest F1 score obtained (81.91) is lower than our best score without external resources.
This can attributed to overfitting of the ensemble on the development data.
It is nonetheless interesting that adding spelling features does boost performance within the external resources setting.

The main confusions of our system were within three groups.
We suggest two reasons for this bias.
On the one hand, the South European group also encompasses only Romance languages, hence the confusion could be attributed to the learners making similar mistakes in the grammar.
However, both the South Asian group and the East Asian group comprise languages which are not related to one another.
Therefore, it is reasonable to assume that the confusion is also due to a cultural bias, such as South European learners using more vacation-related words, or South Asian learners using words related to India (in which both of the languages in question are spoken).

\section{Conclusions}
We describe our system for the NLI Shared Task 2017, which is one the first system to involve a neural approach to this task.
Although deep neural networks are able to perform this task, traditional methods still appear to be better.

\section*{Acknowledgments}
This work was partially funded by the NWO--VICI grant 'Lost in Translation -- Found in Meaning' (288-89-003).


\bibliography{bea12nli}
\bibliographystyle{emnlp_natbib}

\end{document}
