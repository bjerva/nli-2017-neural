%\title{emnlp 2017 instructions}
% File emnlp2017.tex
%

\documentclass[11pt,letterpaper]{article}
\usepackage{emnlp2017}
\usepackage{times}
\usepackage{latexsym}
\usepackage{todonotes}

% Uncomment this line for the final submission:
\emnlpfinalcopy

%  Enter the EMNLP Paper ID here:
\def\emnlppaperid{***}

% To expand the titlebox for more authors, uncomment
% below and set accordingly.
% \addtolength\titlebox{.5in}

\newcommand\BibTeX{B{\sc ib}\TeX}


\title{The RUG-SU system at the NLI shared task}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}
% If the title and author information does not fit in the area allocated,
% place \setlength\titlebox{<new height>} right after
% at the top, where <new height> can be something larger than 2.25in
\author{Johannes Bjerva \and Gintar\.e Grigonyt\.e \and Robert {\"O}stling \and Barbara Plank \\
{\tt j.bjerva@rug.nl\hfill{gintare,robert}@ling.su.se\hfill b.plank@rug.nl}}

\date{}

\begin{document}

\maketitle

\begin{abstract}
    We present the RUG-SU submission at the 2017 shared task on Native
    Language Inference.
\end{abstract}


\section{Introduction}

We used resnets \citep{He2016identity}.

\section{Background}

\section{Data}

\section{Systems}

\subsection{CNN}
\todo[inline]{Johannes, please describe this}

\subsection{Syntactic submodel}
In order to easier capture general syntactic patterns, we use a sentence-level
bidirectional LSTM over tokens and their corresponding part of speech tags
from the Stanford CoreNLP toolkit \citep{Manning2014corenlp}.  PoS tags are
represented by
64-dimensional embeddings, initialized randomly;  word tokens by
300-dimensional embeddings, initialized with GloVe \citep{Pennington2014glove}
embeddings trained on 840 billion words of English web data from the Common
Crawl project.\footnote{ Available at
\url{https://nlp.stanford.edu/projects/glove/}}

To reduce overfitting, we perform training by choosing a random subset of 50\%
of the sentenecs in an essay, concatenating their PoS tag and token
embeddings, and running the resulting vector sequence through a bidirectional
LSTM layer with 256 units per direction. We then average the final output
vector of the LSTM over all the selected sentences from the essay, pass it
through a hidden layer with 1024 units and rectified linear activations, then
make the final predictions through a linear layer with softmax activations.
We apply dropout (50\%) on the final hidden layer.

\subsection{Spelling features}
\todo[inline]{Gintare, please describe this}

\subsection{CBOW features}
\todo[inline]{Barbara, please describe this}

\subsection{Ensemble}
The systems are combined into an ensemble, consisting of a linear SVM.
We use the probability distributions over the labels, as output by each system, as features for the SVM.
The ensemble is trained and tuned on a random subset of the development set ($70/30$ split).
For the selection of systems to include in the ensemble, we use the combination of systems resulting in the highest mean accuracy over five such random splits.


\section{Results}

\section{Analysis}

\section{Discussion}

\section{Conclusions}

\section*{Acknowledgments}

We wish to thank Noam Chomsky for inventing syntax.

\bibliography{bea12nli}
\bibliographystyle{emnlp_natbib}

\end{document}
